import csv
import codecs
import pandas as pd
import numpy as np 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import string
import nltk
from nltk import word_tokenize
from nltk import WordNetLemmatizer
from nltk.util import ngrams
from datetime import datetime
from collections import Counter

tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
### Cleaning the Rep Notes ##
Filtered_Notes_Set= pd.read_csv("Raw_Sales_Sample.csv")
Filtered_Notes_Set = Filtered_Notes_Set.dropna(subset=['phrase'])
corpus = Filtered_Notes_Set['phrase'].values
corpus = corpus[pd.notnull(corpus)]
### Stop Word Removal                
stopfile = open('stopwords.txt','r')
stopwords = stopfile.read().split()
clean_corpus = []
for i in range(len(corpus)):
    clean_list = []
    s = corpus[i].lower().translate(None, string.punctuation)
    tokens = nltk.word_tokenize(s)
    for w in tokens: 
        if w not in stopwords:
            clean_list.append(w)
    clean_corpus.append(' '.join(clean_list))

Filtered_Notes_Set['phrase']=clean_corpus   
Filtered_Notes_Set.to_csv("Clean_Comments.csv");


#Lemmatize the Cleaned Transcripts & Convert words to their root form              
Filtered_Notes_Final = Filtered_Notes_Set
lemma = nltk.WordNetLemmatizer() 
corpus = Filtered_Notes_Final['phrase'].values
corpus = corpus[pd.notnull(corpus)]
corpus = corpus.tolist()
lemmatized_corpus = []
for i in range(len(corpus)):
    lemmatized_words = []     
    s = corpus[i].translate(None, string.punctuation)           
    tokens = nltk.word_tokenize(s)
    for word, tag in nltk.pos_tag(nltk.word_tokenize(s)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemmatized_words.append(word)
        else:
            lemmatized_words.append(lemma.lemmatize(word, wntag))   
    lemmatized_corpus.append(' '.join(lemmatized_words))   
Filtered_Notes_Final['phrase']= lemmatized_corpus   
Filtered_Notes_Final.to_csv("Lemmatized_Comments.csv", index = False)        


corpus = Filtered_Notes_Final['phrase'].values
corpus = corpus[pd.notnull(corpus)]
corpus = corpus.tolist()
corpus = list(set(corpus))

#Defining the Vectorizer
ngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 3), min_df=1)
X = ngram_vectorizer.fit_transform(corpus)

#Defining the Vocabulary
vocab = list(ngram_vectorizer.get_feature_names())
counts = X.sum(axis=0).A1

#Combining Words and their counts
freq_distribution = Counter(dict(zip(vocab, counts)))

with open("C:\Users\A589562\Bag_of_Word_Count.csv", "wb") as f:
    writer = csv.writer(f)
    writer.writerows(freq_distribution.most_common(100))

    
#Bigram Generation    
bigrams = [b for l in corpus for b in zip(l.split(" ")[:-1], l.split(" ")[1:])]  
bigram_freq = Counter(bigrams)

with open("Bag_of_Bigram_Count_Overall_Sales_Q1.csv", "wb") as f:
    writer = csv.writer(f)
    writer.writerows(bigram_freq.most_common(200))


#Trigram Generation    
corpus1 = corpus
corpus1 = ' '.join(corpus1)
tokens = nltk.word_tokenize(corpus1)
trigrams = ngrams(tokens,3)    
freq_trigrams = Counter(trigrams)

with open("Bag_of_Trigram_Count_Overall_Sales_Q4.csv", "wb") as f:
    writer = csv.writer(f)
    writer.writerows(freq_trigrams.most_common(200))
